{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analysis of FAIR metrics\n",
    "\n",
    "In this notebook, the retrieved results of the FAIR metrics collection is analyzed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import date\n",
    "import statistics\n",
    "from collections import Counter\n",
    "from pathlib import Path\n",
    "import os\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from pandas.plotting import table \n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.table as table\n",
    "import seaborn as sns\n",
    "from scipy import stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams['figure.figsize'] = [12, 8]\n",
    "plt.rcParams.update({'font.size': 22})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Folders and constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# subset variable is used to run the notebook for all users, only individual \n",
    "# accounts or organisations (group)\n",
    "subset = \"all\"  # \"user\", \"group\", \"all\"\n",
    "\n",
    "# todo make this into something nice\n",
    "how_fair_is = pd.read_csv(\"results/repositories_howfairis.csv\")\n",
    "languages = pd.read_csv(\"results/languages.csv\")\n",
    "contributors = pd.read_csv(\"results/contributors.csv\")\n",
    "topics = pd.read_csv(\"results/topics.csv\")\n",
    "installations = pd.read_csv(\"results/download_stats.csv\")\n",
    "\n",
    "\n",
    "# file paths\n",
    "fp_repos = Path(\"..\", \"collect_repositories\", \"results\", \"repositories_filtered.csv\")\n",
    "fp_figs = Path(\"figs\")\n",
    "\n",
    "# create output folder if not exists\n",
    "fp_figs.mkdir(exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load filtered repositories\n",
    "Load (manually) filtered repositories into notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "repos = pd.read_csv(fp_repos)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Filtering for research groups or private users only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "users = pd.read_excel(\"../collect_users/results/users_enriched.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_research_group_filter(dataset):\n",
    "    user_is_research_group = []\n",
    "    for row in dataset.iterrows():\n",
    "        try:\n",
    "            owner = row[1]['owner']\n",
    "        except:\n",
    "            owner = row[1]['html_url_repository'].split('/')[-2]\n",
    "            \n",
    "            \n",
    "      \n",
    "        row_users = users.loc[users['user_id'] == owner]\n",
    "        if len(row_users['is_research_group']) == 0:\n",
    "            user_is_research_group.append(False)\n",
    "        for i in row_users['is_research_group']:\n",
    "            if i == 1:\n",
    "                user_is_research_group.append(True)\n",
    "                break\n",
    "            else:\n",
    "                user_is_research_group.append(False)\n",
    "                break\n",
    "                    \n",
    "    return user_is_research_group\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "repos['is_research_group'] = add_research_group_filter(repos)\n",
    "contributors['is_research_group'] = add_research_group_filter(contributors)\n",
    "topics['is_research_group'] = add_research_group_filter(topics)\n",
    "languages['is_research_group'] = add_research_group_filter(languages)\n",
    "how_fair_is['is_research_group'] = add_research_group_filter(how_fair_is)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Subset repositories\n",
    "Make a subset of repositories on \"user\", \"group\", or \"all\".\n",
    "\n",
    "- \"user\" - Account of individual researcher\n",
    "- \"group\" - Account of research group\n",
    "- \"all\" - Both individual researcher or research group"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if subset == \"user\":\n",
    "    repos = repos[~repos['is_research_group']]\n",
    "    contributors = contributors[~contributors['is_research_group']]\n",
    "    topics = topics[~topics['is_research_group']]\n",
    "    languages = languages[~languages['is_research_group']]\n",
    "    how_fair_is = how_fair_is[~how_fair_is['is_research_group']]\n",
    "elif subset == \"group\":\n",
    "    repos = repos[repos['is_research_group']]\n",
    "    contributors = contributors[contributors['is_research_group']]\n",
    "    topics = topics[topics['is_research_group']]\n",
    "    languages = languages[languages['is_research_group']]\n",
    "    how_fair_is = how_fair_is[how_fair_is['is_research_group']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the github API and the Howfairis software, we collected data on the repositories, their FAIRness, the languages used in the repository, the amount of contributors for each repository and the topics mentoined in each repository. \n",
    "\n",
    "The Howfairis software gives  boolean value on five measures of FAIRness for each repository. These five measures consist of having a repository (which is always True for our dataset), having a license, being available in a registry, allowing citation of the software and following the FAIR software quality checklist. Below, the first five rows of the datasets are displayed. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "repos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "how_fair_is"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "languages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "contributors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"The repository data from Github contains the following columns:\")\n",
    "print(repos.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sofware licenses \n",
    "\n",
    "A license is important because it shows that people have permission to use and modify your code. The plots below show the licenses used in the collection. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# short names for licenses\n",
    "licenses_abbr = {\n",
    "    'MIT License': 'MIT',\n",
    "    'GNU General Public License v3.0': 'GPLv3',\n",
    "    'Other': 'Other',\n",
    "    'Apache License 2.0': 'Apache 2.0',\n",
    "    'GNU General Public License v2.0': 'GPLv2',\n",
    "    'BSD 3-Clause \"New\" or \"Revised\" License': 'BSD 3-Clause',\n",
    "    'Creative Commons Attribution 4.0 International': 'CC-BY',\n",
    "    'Creative Commons Zero v1.0 Universal': 'CC0',\n",
    "    'GNU Lesser General Public License v3.0': 'LGPLv3',\n",
    "    'The Unlicense': 'Unlicense',\n",
    "    'GNU Affero General Public License v3.0': 'AGPPLv3',\n",
    "    'BSD 2-Clause \"Simplified\" License': 'BSD 2-Clause',\n",
    "    'Mozilla Public License 2.0': 'MPL 2.0',\n",
    "    'GNU Lesser General Public License v2.1': 'LGPLv2.1',\n",
    "    'Creative Commons Attribution Share Alike 4.0 International': 'CC-BY-SA',\n",
    "    'ISC License': 'ISC'\n",
    "    \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # all licences\n",
    "# repos[\"license\"].value_counts().plot(kind='bar', title = 'All licences')\n",
    "# plt.savefig(Path(fp_figs, f'licenses_all_{subset}.png'), bbox_inches=\"tight\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# top most used 10 licences\n",
    "top_10_licenses = repos[\"license\"].value_counts().head(10)\n",
    "print(top_10_licenses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = sns.barplot(top_10_licenses.index, top_10_licenses)\n",
    "ax.set_xticklabels(ax.get_xticklabels(),rotation = 90)\n",
    "ax.set_ylabel(\"Count\")\n",
    "ax.set_title(\"Top 10 most popular licenses\")\n",
    "\n",
    "plt.savefig(Path(fp_figs, f'licenses_first10_{subset}.png'), bbox_inches=\"tight\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "top_10_licenses_short = repos[\"license\"].replace(licenses_abbr).value_counts().head(10)\n",
    "print(top_10_licenses_short)\n",
    "\n",
    "# first 10 licences with short names\n",
    "ax = sns.barplot(top_10_licenses_short.index, top_10_licenses_short)\n",
    "ax.set_xticklabels(ax.get_xticklabels(),rotation = 90)\n",
    "ax.set_ylabel(\"Count\")\n",
    "ax.set_title(\"Top 10 most popular licenses\")\n",
    "\n",
    "plt.savefig(Path(fp_figs, f'licenses_short_first10_{subset}.png'), bbox_inches=\"tight\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# first 10 licences with short names\n",
    "ax = sns.barplot(top_10_licenses_short.index, top_10_licenses_short / top_10_licenses_short.sum() * 100)\n",
    "ax.set_xticklabels(ax.get_xticklabels(),rotation = 90)\n",
    "ax.set_ylabel(\"Percentage\")\n",
    "ax.set_title(\"Top 10 most popular licenses (in %)\")\n",
    "\n",
    "plt.savefig(Path(fp_figs, f'licenses_short_first10_percentage_{subset}.png'), bbox_inches=\"tight\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below, the propotion of licenses compared to the total is shown, with their absolute count shown below that. . "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "percentage_license = round(float(repos[\"license\"].isna().sum()/len(repos)* 100),2)\n",
    "print(\"{} % of repositories have a license on their page\".format(percentage_license))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Languages\n",
    "\n",
    "The programming language that is used in the repository gives information on the type of project the repository is. data analysis is mostly done in R and Python, though Python is used for other purposes as well. \n",
    "\n",
    "### Jupyter Notebooks\n",
    "\n",
    "Please note that due to the way GitHub calculates the dominant language in a repository, Jupyter Notebooks tend to be the top language if used in a repository. The top language is determined by the number of characters, which is usually much larger in Jupyter Notebook files compared to for example Python scripts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "repos[\"language\"].value_counts().plot.bar(figsize = (100,50), fontsize = 100)\n",
    "plt.title(\"Programming languages by popularity\", fontsize = 100)\n",
    "plt.savefig(Path(fp_figs, f'language_{subset}.png'), bbox_inches=\"tight\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "repos[\"language\"].value_counts().head(20).plot.barh(figsize = (50,50), fontsize = 100)\n",
    "plt.title(\"Top 20 Programming languages\", fontsize = 100)\n",
    "plt.gca().invert_yaxis()\n",
    "plt.savefig(Path(fp_figs, f'language_top20_{subset}.png'), bbox_inches=\"tight\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_10_languages = repos[\"language\"].value_counts().head(10)\n",
    "\n",
    "ax = sns.barplot(top_10_languages.index, top_10_languages / top_10_languages.sum() * 100)\n",
    "ax.set_xticklabels(ax.get_xticklabels(),rotation = 90)\n",
    "ax.set_ylabel(\"Percentage\")\n",
    "ax.set_title(\"Top 10 most popular languages (in %)\")\n",
    "\n",
    "plt.savefig(Path(fp_figs, f'language_top10_{subset}.png'), bbox_inches=\"tight\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "repo_languages_rel = repos[\"language\"].value_counts().head(10) / len(repos)\n",
    "repo_languages_rel.plot.bar(figsize = (50,25), fontsize = 50)\n",
    "plt.title(\"Top 10 programming languages (in %)\", fontsize = 50)\n",
    "plt.savefig(Path(fp_figs, f'language_top10_percentage_{subset}.png'), bbox_inches=\"tight\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Relative occurence languages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "repo_languages_rel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The following percentages represent the percentage of repositories that contains that specific language for the 10 most occuring languages:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for value, language in zip(languages[\"language\"].value_counts().head(10), languages[\"language\"].value_counts().head(10).keys()):\n",
    "    percentage = value / len(repos) * 100\n",
    "    print(\"{} is present in {:.2f} % of all repositories\". format(language, percentage))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "languages_rel = languages[\"language\"].value_counts().head(10) / len(repos)\n",
    "languages_rel.plot.bar(figsize = (50,25), fontsize = 50)\n",
    "plt.title(\"Top 10 occuring programming languages (in %)\", fontsize = 50)\n",
    "plt.savefig(Path(fp_figs, f'occurence_language_top10_percentage_{subset}.png'), bbox_inches=\"tight\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "languages[\"user\"] = languages[\"html_url_repository\"].map(lambda url: url.split(\"/\")[3]) # extract user from url\n",
    "languages[\"repo\"] = languages[\"html_url_repository\"].map(lambda url: url.split(\"/\")[4]) # extract user from url\n",
    "unique_users = set(languages[\"user\"])\n",
    "userlang_chars = dict()\n",
    "for user in unique_users:\n",
    "    repos_user = languages.loc[languages['user'] == user]\n",
    "    repos_user_total_chars = sum(repos_user[\"num_chars\"])\n",
    "    for index, row in repos_user.iterrows():\n",
    "        key = row[1] # select language\n",
    "        if key not in userlang_chars:\n",
    "            userlang_chars[key] = row[\"num_chars\"]/repos_user_total_chars\n",
    "        else:\n",
    "            userlang_chars[key] += row[\"num_chars\"]/repos_user_total_chars"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The following shows a plot with stacked percentage of language usage of a user\n",
    "#### E.g.: User X has 100 repos written in Python --> Add 1.0 to Python bar. User Y has 1 repo written in R and 1 in C# --> Add 0.5 to R and 0.5 to C#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "userlang_df = pd.DataFrame.from_dict(userlang_chars, orient=\"index\", columns=['stacked_percentage'])\n",
    "userlang_df.drop([\"HTML\", \"TeX\"], inplace = True)\n",
    "userlang_df.sort_values(by=\"stacked_percentage\", ascending = False, inplace = True)\n",
    "userlang_df_top10 = userlang_df.head(10)\n",
    "print(userlang_df_top10)\n",
    "\n",
    "ax = sns.barplot(userlang_df_top10.index, userlang_df_top10[\"stacked_percentage\"] / userlang_df_top10[\"stacked_percentage\"].sum() * 100)\n",
    "ax.set_xticklabels(ax.get_xticklabels(),rotation = 90)\n",
    "ax.set_ylabel(\"Percentage\")\n",
    "ax.set_title(\"Top 10 most popular languages by user (in %)\")\n",
    "\n",
    "plt.savefig(Path(fp_figs, f'most_occuring_languages_stacked_percent_top10_{subset}.png'), bbox_inches=\"tight\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Correlation table top 10 languages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "languages[\"language\"].value_counts().head(10).keys()\n",
    "dict_languages_user = Counter()\n",
    "dict_top_languages = dict()\n",
    "for language in languages[\"language\"].value_counts().head(10).keys():\n",
    "    dict_top_languages[language] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_users = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for html_url in repos['html_url']: \n",
    "    #print(html_url)\n",
    "    rows_language_for_one_user = languages.loc[languages['html_url_repository'] == html_url]\n",
    "    dict_languages_user = Counter()\n",
    "    for language in languages[\"language\"].value_counts().head(10).keys(): #set all values of languages that need to be present to create a dataset in a later stage to zero\n",
    "        dict_languages_user[language] = 0\n",
    "    for language in rows_language_for_one_user['language']:\n",
    "        if language in dict_languages_user: #only count languages in the top 10\n",
    "            dict_languages_user[language] += 1\n",
    "    dict_users[html_url] = dict_languages_user #each user has a dict with 10 booleans representing whether the top 10 language is present in their repo or not\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "languages_per_repo = pd.DataFrame.from_dict(dict_users, orient = 'index')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "languages_per_repo.corr().round(decimals = 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Topics\n",
    "\n",
    "Topics describe the context of the repository. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topics['topic'].value_counts().nlargest(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topics['topic'].value_counts().nlargest(10).plot(kind = 'bar', title = 'Top 10 topics')\n",
    "plt.savefig(Path(fp_figs, f'most_occuring_topics_top10_{subset}.png'), bbox_inches=\"tight\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Number of  contributors\n",
    "\n",
    "The number of contributors gives information on how many people put effort into the repository. Theoratically, it would make sense if a higher amount of contributors implied a more FAIR repository, because those are easier to find in a registery and work on because of their license. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "contributors['html_url_repository'].value_counts().plot.hist(x = 'contributors', figsize = (10,10), fontsize = 12, title = 'Number of contributors')\n",
    "plt.savefig(Path(fp_figs, f'contributors_{subset}.png'), bbox_inches=\"tight\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "contributors['html_url_repository'].value_counts().nlargest(20, keep = 'all')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('mean contributors: {:.2f}'.format(statistics.mean(contributors['html_url_repository'].value_counts())))\n",
    "print('median contributors: {}'.format(statistics.median(contributors['html_url_repository'].value_counts())))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Howfairis sum score descriptives and correlations\n",
    "\n",
    "In this section, the median and meand of the howfairis sum score is shown, as well as correlations the how fair is sum score, the amount of contributors and the amount of months since the last commit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "how_fair_sum_scores = []\n",
    "for index, row in how_fair_is.iterrows():\n",
    "    how_fair_is_sum = row['howfairis_repository'] + row['howfairis_license'] +row['howfairis_registry'] + row['howfairis_citation'] + row['howfairis_checklist']\n",
    "    how_fair_sum_scores.append(how_fair_is_sum)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "how_fair_is['how_fair_sum_scores'] = how_fair_sum_scores\n",
    "print(\"mean: {:.2f}\".format(statistics.mean(how_fair_sum_scores)))\n",
    "print(\"median: {}\".format(statistics.median(how_fair_sum_scores)))\n",
    "pd.DataFrame((statistics.mean(how_fair_sum_scores), statistics.median(how_fair_sum_scores)),[\"mean\", \"median\"])\n",
    "how_fair_is.boxplot(column = 'how_fair_sum_scores')\n",
    "plt.title('boxplot of the howfairis sumscore')\n",
    "plt.savefig(Path(fp_figs, f'howfairissum_boxplot_{subset}.png'), bbox_inches=\"tight\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# how_fair_is['how_fair_sum_scores'].value_counts().sort_index().plot.bar(title = 'Scores howfairis')\n",
    "# plt.savefig(Path(fp_figs, f'howfairissum_histogram_{subset}.png'), bbox_inches=\"tight\")\n",
    "\n",
    "\n",
    "howfairis_result = how_fair_is['how_fair_sum_scores'].value_counts().sort_index()\n",
    "print(howfairis_result)\n",
    "\n",
    "ax = sns.barplot(howfairis_result.index, howfairis_result / howfairis_result.sum() * 100, palette='Blues_d')\n",
    "ax.set_xticklabels(ax.get_xticklabels(),rotation = 90)\n",
    "ax.set_ylabel(\"Percentage\")\n",
    "ax.set_title(\"Score in howfairis tool (in %)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "correlation_df = pd.DataFrame()\n",
    "months_ago_most_recent_commit =[]\n",
    "today = str(date.today())\n",
    "split_today = today.split('-')\n",
    "for date_item in repos['updated_at']:\n",
    "    split_date = date_item.split('-')\n",
    "    years_ago = int(split_today[0]) - int(split_date[0])\n",
    "    months_ago = 12 * years_ago + (int(split_today[1]) - int(split_date[1]))\n",
    "    months_ago_most_recent_commit.append(months_ago)\n",
    "repos['months_ago_most_recent_commit'] = months_ago_most_recent_commit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "correlation_df['how_fair_sum_scores'] = how_fair_sum_scores\n",
    "correlation_df['contributions']= contributors['contributions']\n",
    "correlation_df['months_ago_most_recent_commit'] = repos['months_ago_most_recent_commit']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "correlation_df.corr(method='pearson').round(decimals=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_users = set(how_fair_is[\"owner\"])\n",
    "unique_users\n",
    "howfairis_max_user = list()\n",
    "howfairis_average = list()\n",
    "for user in unique_users:\n",
    "    repos_user = how_fair_is.loc[how_fair_is['owner'] == user]\n",
    "    howfairis_max_user.append(max(repos_user[\"how_fair_sum_scores\"]))\n",
    "    num_repos = len(repos_user.index)\n",
    "    average = sum(repos_user[\"how_fair_sum_scores\"])/num_repos\n",
    "    howfairis_average.append([average, num_repos])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(howfairis_max_user, bins = range(1,7))\n",
    "plt.xticks(range(1,6))\n",
    "plt.ylabel('Count')\n",
    "plt.xlabel('Howfairis score')\n",
    "plt.title(\"Max howfairis score per user aggregated, \" + str(len(unique_users)) + \" users\")\n",
    "plt.savefig(Path(fp_figs, f'howfairis_stacked_max{subset}.png'), bbox_inches=\"tight\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(*zip(*howfairis_average))\n",
    "plt.ylabel('Number of repositories')\n",
    "plt.xlabel('Average howfairis score')\n",
    "plt.title(\"Average howfairis score per user, \" + str(len(unique_users)) + \" users\")\n",
    "plt.savefig(Path(fp_figs, f'howfairis_scatter_repos_avg{subset}.png'), bbox_inches=\"tight\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Binary correlation plot between howfairis variables\n",
    "\n",
    "A correlation table that shows the correlation between the howfairis measures."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "correlation_how_fair_is = pd.DataFrame()\n",
    "\n",
    "#correlation_how_fair_is['howfairis_repository'] = how_fair_is['howfairis_repository']\n",
    "correlation_how_fair_is['howfairis_license'] = how_fair_is['howfairis_license']\n",
    "correlation_how_fair_is['howfairis_registry'] = how_fair_is['howfairis_registry']\n",
    "correlation_how_fair_is['howfairis_citation'] = how_fair_is['howfairis_citation']\n",
    "correlation_how_fair_is['howfairis_checklist'] = how_fair_is['howfairis_checklist']\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "correlation_how_fair_is.corr(method='pearson').round(decimals=2) #howfairis_repository is all true, so has no variance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Programming language and Howfairis\n",
    "\n",
    "Different programming languages need different licences. For some languages (e.g. python), these are more complex. How does this influence the HowFairis score?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index_how_fair_is = 0\n",
    "language_how_fair_is_df = []\n",
    "#get a list with the how-fair-is scores added to the language dataset\n",
    "for index,row in languages.iterrows():\n",
    "    if how_fair_is['html_url'][index_how_fair_is] != languages['html_url_repository'][index]:\n",
    "        index_how_fair_is += 1 \n",
    "    sum_score = how_fair_is['how_fair_sum_scores'][index_how_fair_is]\n",
    "    language_how_fair_is_df.append(sum_score)\n",
    "regression_data_how_fair_is_language = pd.DataFrame()\n",
    "language_data_dummies = pd.get_dummies(languages['language'])  #one hot-encoding for language dataset\n",
    "regression_data_how_fair_is_language['how_fair_is_sum'] = language_how_fair_is_df\n",
    "languages['how_fair_sum_scores'] = regression_data_how_fair_is_language #add scores to language dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_size = int(len(languages) * 0.8) #training size is 2227, testing size is  557\n",
    "regression_how_fair_is_language_train_X = language_data_dummies[:train_size]\n",
    "regression_how_fair_is_language_train_y = regression_data_how_fair_is_language['how_fair_is_sum'][:train_size]\n",
    "regression_how_fair_is_language_test_X = language_data_dummies[train_size:]\n",
    "regression_how_fair_is_language_test_y = regression_data_how_fair_is_language['how_fair_is_sum'][train_size:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train regression\n",
    "regression_language_how_fair_is = LinearRegression()\n",
    "regression_language_how_fair_is.fit(regression_how_fair_is_language_train_X, regression_how_fair_is_language_train_y)\n",
    "\n",
    "# Make predictions using the testing set\n",
    "language_how_fair_is_y_pred = regression_language_how_fair_is.predict(regression_how_fair_is_language_test_X)\n",
    "\n",
    "# The coefficients\n",
    "#print('Coefficients: \\n', regression_language_how_fair_is.coef_)\n",
    "\n",
    "# The mean squared error\n",
    "print('Mean squared error: %.2f'\n",
    "      % mean_squared_error(regression_how_fair_is_language_test_y, language_how_fair_is_y_pred))\n",
    "# The coefficient of determination: 1 is perfect prediction\n",
    "print('Coefficient of determination: %.2f'\n",
    "      % r2_score(regression_how_fair_is_language_test_y, language_how_fair_is_y_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "language_subset = language_data_dummies[['Python', 'Shell', 'R', 'JavaScript', 'HTML']] #top 5 languages\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_size = int(len(language_subset) * 0.8)\n",
    "regression_how_fair_is_language_train_X = language_subset[:train_size]\n",
    "regression_how_fair_is_language_train_y = regression_data_how_fair_is_language['how_fair_is_sum'][:train_size]\n",
    "regression_how_fair_is_language_test_X = language_subset[train_size:]\n",
    "regression_how_fair_is_language_test_y = regression_data_how_fair_is_language['how_fair_is_sum'][train_size:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train regression\n",
    "regression_language_how_fair_is = LinearRegression()\n",
    "regression_language_how_fair_is.fit(regression_how_fair_is_language_train_X, regression_how_fair_is_language_train_y)\n",
    "\n",
    "# Make predictions using the testing set\n",
    "language_how_fair_is_y_pred = regression_language_how_fair_is.predict(regression_how_fair_is_language_test_X)\n",
    "\n",
    "# The coefficients\n",
    "print('Coefficients: \\n', regression_language_how_fair_is.coef_)\n",
    "\n",
    "# The mean squared error\n",
    "print('Mean squared error: %.2f'\n",
    "      % mean_squared_error(regression_how_fair_is_language_test_y, language_how_fair_is_y_pred))\n",
    "# The coefficient of determination: 1 is perfect prediction\n",
    "print('Coefficient of determination: %.2f'\n",
    "      % r2_score(regression_how_fair_is_language_test_y, language_how_fair_is_y_pred))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A simple regression for predicting the how fair is score from  all or a subset of languages does not seem to yield any results "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Examples of high howfairis scores \n",
    "\n",
    "In this section, some links to the pages that have a high howfairis score are shown. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "how_fair_is_high_scores = how_fair_is[how_fair_is['how_fair_sum_scores'] > 3].sort_values('how_fair_sum_scores', ascending=False)\n",
    "\n",
    "how_fair_is_high_scores[['html_url', 'how_fair_sum_scores']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Re-use of other projects and articles\n",
    "\n",
    "Are there competitors?\n",
    "How do they relate to each other?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Installation statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "installations[installations[\"repository_name\"] != \"pcalg\"].sort_values(\"last_month\", ascending=False).reset_index(drop=True).drop(\"date\", axis=1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
